# Neural Language Model Training (PyTorch)

### ğŸ“˜ Objective  
Train a Neural Language Model from scratch using PyTorch to understand how model architecture and training affect text prediction and perplexity.

---

### ğŸ§¾ Dataset  
**Dataset:** *Pride and Prejudice â€“ Jane Austen* (provided by IIIT Hyderabad)  
- Total characters: ~711k  
- Vocabulary size: 3,164 (after min_freq=3)  
- Train/validation split: 90/10  

---

### âš™ï¸ Implementation Overview  
**Architecture:** 2-layer LSTM Language Model  
- Embedding size: 128  
- Hidden size: 256  
- Dropout: 0.3  
- Sequence length: 30  
- Batch size: 64  
- Optimizer: Adam (lr=0.001)  
- Loss: CrossEntropyLoss  
- Metric: Perplexity (PPL)

---

### ğŸ§  Experiments  
| Model Type | Configuration | Epochs | Observation |
|-------------|---------------|---------|--------------|
| **Underfit** | Small LSTM (64 hidden, 1 layer) | 2 | High loss, flat curves |
| **Overfit** | Large LSTM (512 hidden, 3 layers, no dropout) | 10 | Low train loss, high val loss |
| **Best-fit** | Medium LSTM (256 hidden, 2 layers, dropout=0.3) | 5 | Balanced train/val losses |

---

### ğŸ“Š Results Summary  
| Model | Train Loss | Val Loss | Val PPL | Remarks |
|--------|-------------|-----------|----------|----------|
| Underfit | ~5.8 | ~5.9 | ~365 | Small capacity |
| Overfit | ~3.9 | ~5.6 | ~270 | Memorization observed |
| Best-fit | ~4.2 | ~5.3 | ~200 | Best generalization |

---

### ğŸ’¬ Sample Generated Text (Best-fit Model)
Elizabeth was a great deal of the room and the first of the room and the first of the room and the first of the room...


> The model demonstrates structural learning and repetition typical of early-epoch LSTMs. Longer training reduced `<unk>` tokens and improved fluency.

---
## ğŸ“ Folder Structure

```
IIIT_Assignment2/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Pride_and_Prejudice-Jane_Austen.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_utils.py
â”‚   â”œâ”€â”€ model.py
â”‚   â””â”€â”€ train.py
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ loss_plot.png
â”‚   â”œâ”€â”€ model_best.pth
â”‚   â”œâ”€â”€ model_bestfit_long.pth
â”‚   â”œâ”€â”€ model_bestfit.pth
â”‚   â”œâ”€â”€ model_overfit.pth
â”‚   â””â”€â”€ model_underfit.pth
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ assignment2.ipynb
â””â”€â”€ Assignment_report.pdf
```

### ğŸš€ How to Run
1. Clone the repository and install dependencies:
   ```bash
   pip install torch matplotlib tqdm
2. Ensure dataset is placed inside data/.

3. Run the training:
python src/train.py

4.View results and plots in the results/ folder.

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/sisira19/Neural-Language-Model-Training-PyTorch-.git
cd Neural-Language-Model-Training-PyTorch-
```

### 2ï¸âƒ£ Install Dependencies
```bash
pip install torch numpy matplotlib tqdm
```

---

## ğŸ‹ï¸â€â™‚ï¸ Training Instructions

Run training for different configurations:

### **Underfit**
```bash
python src/train.py --config underfit
```

### **Overfit**
```bash
python src/train.py --config overfit
```

### **Best-fit**
```bash
python src/train.py --config bestfit
```

The script will:

âœ”ï¸ Load dataset  
âœ”ï¸ Train the model  
âœ”ï¸ Save weights to `results/`  
âœ”ï¸ Save loss plots  

---

## ğŸ§ª Text Generation (Inference)

Use a trained model to generate text:

```bash
python src/train.py --generate --model_path results/model_bestfit.pth
```

---

## ğŸ”— Download Trained Models (Google Drive)

All trained models are available in a public Google Drive folder:

ğŸ“ **Google Drive (Models + Results):**  
https://drive.google.com/drive/folders/15Dvyty1zYdVIHajjTq-6R2A-5SJqzCkc?usp=drive_link

This includes:

- `model_underfit.pth`  
- `model_overfit.pth`  
- `model_bestfit.pth`  
- Additional experimental files  
- Loss plots  

---

## ğŸ“Š Results Summary

The `results/` folder includes:

- `loss_plot.png` â€” combined loss visualization  
- Underfit / Overfit / Best-fit model weights  
- Trained model variants (`best`, `bestfit_long`)  

These plots help compare convergence behavior and model quality.

---

## â­ Extra Credit Work

This project includes:

- Multiple model experiments  
- Proper training pipeline  
- Organized code structure  
- Loss visualizations  
- Reproducible results  
- Additional long-training best-fit model (`model_bestfit_long.pth`)  

---

## ğŸ‘¨â€ğŸ’» Author
**Sisira**
