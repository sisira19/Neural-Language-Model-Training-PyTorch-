# Neural-Language-Model-Training-PyTorch-
# IIIT Hyderabad â€“ Assignment 2  
## Neural Language Model Training (PyTorch)

### ğŸ“˜ Objective  
Train a Neural Language Model from scratch using PyTorch to understand how model architecture and training affect text prediction and perplexity.

---

### ğŸ§¾ Dataset  
**Dataset:** *Pride and Prejudice â€“ Jane Austen* (provided by IIIT Hyderabad)  
- Total characters: ~711k  
- Vocabulary size: 3,164 (after min_freq=3)  
- Train/validation split: 90/10  

---

### âš™ï¸ Implementation Overview  
**Architecture:** 2-layer LSTM Language Model  
- Embedding size: 128  
- Hidden size: 256  
- Dropout: 0.3  
- Sequence length: 30  
- Batch size: 64  
- Optimizer: Adam (lr=0.001)  
- Loss: CrossEntropyLoss  
- Metric: Perplexity (PPL)

---

### ğŸ§  Experiments  
| Model Type | Configuration | Epochs | Observation |
|-------------|---------------|---------|--------------|
| **Underfit** | Small LSTM (64 hidden, 1 layer) | 2 | High loss, flat curves |
| **Overfit** | Large LSTM (512 hidden, 3 layers, no dropout) | 10 | Low train loss, high val loss |
| **Best-fit** | Medium LSTM (256 hidden, 2 layers, dropout=0.3) | 5 | Balanced train/val losses |

---

### ğŸ“Š Results Summary  
| Model | Train Loss | Val Loss | Val PPL | Remarks |
|--------|-------------|-----------|----------|----------|
| Underfit | ~5.8 | ~5.9 | ~365 | Small capacity |
| Overfit | ~3.9 | ~5.6 | ~270 | Memorization observed |
| Best-fit | ~4.2 | ~5.3 | ~200 | Best generalization |

---

### ğŸ’¬ Sample Generated Text (Best-fit Model)
Elizabeth was a great deal of the room and the first of the room and the first of the room and the first of the room...


> The model demonstrates structural learning and repetition typical of early-epoch LSTMs. Longer training reduced `<unk>` tokens and improved fluency.

---
## ğŸ“ Folder Structure

```
IIIT_Assignment2/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Pride_and_Prejudice-Jane_Austen.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_utils.py
â”‚   â”œâ”€â”€ model.py
â”‚   â””â”€â”€ train.py
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ loss_plot.png
â”‚   â”œâ”€â”€ model_best.pth
â”‚   â”œâ”€â”€ model_bestfit_long.pth
â”‚   â”œâ”€â”€ model_bestfit.pth
â”‚   â”œâ”€â”€ model_overfit.pth
â”‚   â””â”€â”€ model_underfit.pth
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ assignment2.ipynb
â””â”€â”€ Assignment_report.pdf
```

```




---

### ğŸš€ How to Run
1. Clone the repository and install dependencies:
   ```bash
   pip install torch matplotlib tqdm
2. Ensure dataset is placed inside data/.

3. Run the training:
python src/train.py

4.View results and plots in the results/ folder.
